# -*- coding: utf-8 -*-
"""Copy of Iterative Clustering LIDC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTmAMp9_97B2qMA2e-iIfx8sXCKhO53e
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, silhouette_samples
import sklearn as sk
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from statistics import mean
import math
import statistics

path = input("Are you Katherine (K) or someone else (S)?")
if(path == 'K'):
  df = pd.read_csv("/content/drive/MyDrive/MedIX - Subtype Discovery Project/Code/notebooks we are actually using LOL/SPIE Extended Abstract Results/Copy of MaxSlices_newMode_2022.csv")
elif path == 'S':
  pathSomeoneElse = input("Please copy the path where you have the MaxSlicesPerNodule file and paste that path here:")
  df = pd.read_csv(pathSomeoneElse)

df = df.drop(['RadiologistID', 'SeriesInstanceUid', 'StudyInstanceUID', 'FilePath', 'DicomImage', 'imageSOP_UID','coords', 'Subtlety_1', 'InternalStructure_1', 'Sphericity_1', 'Margin_1', 'Lobulation_1', 'Spiculation_1', 'Texture_1', 'Malignancy_1', 'Subtlety_2', 'InternalStructure_2', 'Sphericity_2', 'Margin_2', 'Lobulation_2', 'Spiculation_2', 'Texture_2', 'Malignancy_2', 'Subtlety_3', 'InternalStructure_3', 'Sphericity_3', 'Margin_3', 'Lobulation_3', 'Spiculation_3', 'Texture_3', 'Malignancy_3', 'Subtlety_4', 'InternalStructure_4', 'Sphericity_4', 'Margin_4', 'Lobulation_4', 'Spiculation_4', 'Texture_4', 'Malignancy_4', 'Subtlety', 'InternalStructure', 'Sphericity', 'Margin', 'Lobulation', 'Spiculation', 'Texture'], axis=1)

df = df.drop(['Calcification_1', 'Calcification_2', 'Calcification_3', 'Calcification_4', 'Calcification'], axis = 1)

lidc = df

lidc = lidc[lidc.Malignancy != 3.0]
for i, row in lidc.iterrows():
    ifor_val = 0
    if row['Malignancy'] == 4.0 or row['Malignancy'] == 5.0:
      ifor_val = 1
    lidc.at[i,'Malignancy'] = ifor_val

!pip install umap-learn
import umap.umap_ as umap
X_with_id = lidc
y = (lidc['Malignancy']).to_numpy()
X = X_with_id.drop(['noduleID', 'Malignancy'], axis = 1)
print(X.columns)
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
pca = umap.UMAP()
X = pca.fit_transform(X)

print(len(X)) #Length reduced because indeterminate nodules were removed

def createList(r1, r2):
    return [item for item in range(r1, r2)]
# Driver Code
r1, r2 = 0, len(y)
lstOfIds = createList(r1, r2)

def calculateHomogeneity(predicted, actual, origNumLabels = 2):

  numPredictedClusters = len(np.unique(predicted)) # number of predicted clusters
  numActual = len(np.unique(actual)) # number of actual ground truth classes

  clusterHomogeneities = []
  clusterTotals = []
  for i in range(numPredictedClusters):
    homogeneityList = np.zeros(origNumLabels)
    # find all of the indices of the cluster
    ind = np.where(np.isin(predicted,i))
    actual = np.array(actual)
    actualVals = actual[ind] # finding the actual values in that cluster
    totalNumInCluster = len(actualVals) # finding the amount of points in that cluster in order to divide
    clusterTotals.append(totalNumInCluster)
    for i in actualVals:
      homogeneityList[i-1] = homogeneityList[i-1] + 1
    homogeneityList = homogeneityList / totalNumInCluster
    clusterHomogeneities.append(max(homogeneityList))
  return [clusterHomogeneities, clusterTotals]

def iterativeClustering(threshold, data, actual, IdList, centers = [], properlyClassified = [], hsOfRemoved = [], lengthOfData = len(X)):
 
  print("Number of data points to work with: ", data.shape[0])

  #finding the best k

  k = 1;
  currHomogeneity = 0.0;
  while (currHomogeneity < threshold and k <= math.sqrt(lengthOfData/2)):
    k = k + 1
    model = sk.cluster.KMeans(n_clusters = k)
    clusterLabels = model.fit_predict(data)
    clusterCenters = model.cluster_centers_ # getting the centers of the clusters
    homogeneityList = calculateHomogeneity(clusterLabels, actual) 
    currHomogeneity = np.max(homogeneityList[0])
  

  predLabels = clusterLabels
  homogeneityPCluster = homogeneityList[0]
  
  for i in range(k):
    print("Homogeneity of cluster {} is {}".format(i,homogeneityPCluster[i]))
  
  #find clusters to remove
  clustersToRemove = []
  bestClusterHS = currHomogeneity
  print("this is the maximum homogeneity: {}".format(bestClusterHS))
  indexOfMax = homogeneityPCluster.index(bestClusterHS) 
  clusterTotals = homogeneityList[1]
  for i in range(k):
    diffBtwnBest = bestClusterHS - homogeneityPCluster[i]
    
    if i == indexOfMax:
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)
      continue
    if diffBtwnBest < .03 and homogeneityPCluster[i] >= threshold and clusterTotals[i]> math.sqrt(len(data)):
      
      clustersToRemove.append(i)
      
      print("Cluster sectioned off:", i)

  pointsRemoved = 0
  totalPoints = sum(clusterTotals)
  for i in range(k):
    if i in clustersToRemove:
      pointsRemoved += clusterTotals[i]

  
  remainingDatapoints = totalPoints - pointsRemoved #check total points
  print("percent removed: {}".format(pointsRemoved/lengthOfData))
  if remainingDatapoints < math.sqrt(lengthOfData) or (pointsRemoved <= math.sqrt(len(data))): 
    if remainingDatapoints < math.sqrt(lengthOfData):
      print("Too few remaining points.")
    else:
      print("Two few points in a cluster.")
    print("DONE - we have all clusters")
    return list(properlyClassified), list(IdList), list(hsOfRemoved), list(centers)

  else: 
    #remove these dataPoints in cluster chosen to be separated
    #recluster remaining
    IdList = np.array(IdList)

    for num in clustersToRemove:
      indices = np.where(np.isin(predLabels,num))
      toAppend = IdList[indices]
      properlyClassified.append(toAppend)
      centers.append(clusterCenters[num])
      hsOfRemoved.append(homogeneityPCluster[num])
      data = np.delete(data, indices, axis=0)
      actual = np.delete(actual, indices)
      IdList = np.delete(IdList,indices)
      predLabels = np.delete(predLabels,indices)
    
    print("Good clusters removed. Starting over with smaller dataset.")
    return iterativeClustering(threshold, data, actual,IdList, centers, properlyClassified, hsOfRemoved)

separatedClusters, remainingPoints, scores, centers = iterativeClustering(.8, X,y,lstOfIds)

# checking the amount of subtypes and the amount of unclustered points
print(len(separatedClusters))
print(len(remainingPoints))

# checking number of points in a cluster
for i in separatedClusters:
  print(len(i))

#finding nodule IDs of the closest points to cluster centers, in case you want to visualize the images of the clusters
from sklearn.metrics import pairwise_distances_argmin_min
closest, _ = pairwise_distances_argmin_min(centers, X)
for i in closest:
  print(lidc.iloc[i]['noduleID'])

#Calculating homogeneity of unclustered points
labels = y
numLabelsPerClass = {}

for i in remainingPoints:
  label = labels[i]
  if label in numLabelsPerClass:
    numLabelsPerClass[label] += 1
  else:
    numLabelsPerClass[label] = 1

hsPClass = [] #HS per class in unclustered
for key in numLabelsPerClass:
  homogeneity = (numLabelsPerClass[key]/len(remainingPoints))
  hsPClass.append(homogeneity)

hsWoUnclustered = scores
print(scores)
print(mean(scores))
hsWUnclustered = hsWoUnclustered
hsWUnclustered.append(max(hsPClass))
print(mean(hsWUnclustered))
print(hsWUnclustered)

model = sk.cluster.KMeans(n_clusters = len(separatedClusters))
clusterLabels = model.fit_predict(X)
centersForTrad = model.cluster_centers_
# Homogeneity Scores for Traditional Clustering w/ num subtypes from Iterative
lst = calculateHomogeneity(clusterLabels,y)
print(lst[0])
print(len(lst[0]))
print("Average homogeneity for TRADITIONAL clustering: ", mean(lst[0]))

# Finds labels in the dataframe of separated and remaining points

labelsForEverything = np.zeros(len(X))
labelForRemaining = len(separatedClusters)

for label in range(len(separatedClusters)):
  for index in separatedClusters[label]:
    labelsForEverything[index] = label


for index in remainingPoints: 
  labelsForEverything[index] = labelForRemaining

indices = np.where(labelsForEverything==labelForRemaining)
print(indices)

copyOfX = X
XRemoved = np.delete(copyOfX,indices[0],axis=0)

copyOfEverything = labelsForEverything
labelsForRemoved = np.delete(copyOfEverything,indices[0])

def getIds(X_test):
  indexLst = []
  for lst in X_test:
    index = int(lst[-1])
    indexLst.append(index)
  return indexLst

# custom metric to define the number of points misclassified per cluster, using majority class in the cluster as the label
from scipy import stats
def findMisclassified(X, X_test, y, y_test,listOfIndices): 
  numMisclassified = []
  indicesInX = listOfIndices
  groundTruths = [y[i] for i in indicesInX]
  for i in np.unique(y_test):
    numMisclassifiedPerCluster = 0
    indicesOfInterest = np.where(y_test == i)[0]
    truthLabels = [groundTruths[i] for i in indicesOfInterest]
    majorityClassLabel = int(stats.mode(truthLabels)[0])
    for i in truthLabels:
      if i!=majorityClassLabel:
        numMisclassifiedPerCluster = numMisclassifiedPerCluster + 1
    numMisclassified.append(numMisclassifiedPerCluster)
  return numMisclassified

from sklearn.metrics import precision_recall_fscore_support, accuracy_score, balanced_accuracy_score, recall_score, precision_score

r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X1 = pd.DataFrame(X)
X1['ID'] = ids
X1 = X1.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X1, labelsForEverything, test_size = 0.2, stratify = labelsForEverything)
lstOfIndices = getIds(X_test)
X_train1 = []
for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
X_train1 = np.array(X_train1)
X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])
X_test1 = np.array(X_test1)
svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuraciesIterative = matrix.diagonal()/matrix.sum(axis = 1)
print(matrix)
print("Overall accuracy for our ITERATIVE subtypes: ", accuracy_score(y_test, svmResults))
print("Overall Recall for our ITERATIVE subtypes: ", recall_score(y_test, svmResults, average = 'macro'))
print("Overall Precision for our ITERATIVE subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Overall Misclassified for our ITERATIVE subtypes: ", mean(findMisclassified(X, X_test1, y, y_test,lstOfIndices)))
misclassIterative = findMisclassified(X, X_test1, y, y_test,lstOfIndices)

precisionIterative = precision_score(y_test, svmResults, average = None)
recallIterative = recall_score(y_test, svmResults, average = None)
print("Accuracies per class for our ITERATIVE subtypes: ", accuraciesIterative)
print("Precision per class for our ITERATIVE subtypes: ", precisionIterative)
print("Recall per class for our ITERATIVE subtypes: ", recallIterative)
print("Number of points Misclassified per cluster: ", findMisclassified(X, X_test1, y, y_test,lstOfIndices))

# Classify MNIST into ground truth classes (no clustering)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
svm = SVC()
svm.fit(X_train, y_train)
svmResults = svm.predict(X_test)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis=1)
print("Accuracies per class for a simple SVM classifer: ", accuracies)
print("Overall accuracy for a simple SVM classifier: ", accuracy_score(y_test, svmResults))
print("Recall for a simple SVM classifier: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for a simple SVM classifier: ", precision_score(y_test, svmResults, average = 'macro'))

numberOfTradClusters = len(separatedClusters)
#classifying MNIST into n classes using traditional k-means clusters
model = sk.cluster.KMeans(n_clusters = numberOfTradClusters)
clusterLabels = model.fit_predict(X)
centersForTrad = model.cluster_centers_
r1, r2 = 0, len(list(y))
ids = createList(r1, r2)
X1 = pd.DataFrame(X)
X1['ID'] = ids
X1 = X1.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X1, clusterLabels, test_size = 0.2, stratify = clusterLabels)
lstOfIndices = getIds(X_test)
X_train1 = []
for i in range(len(X_train)):
  everythingButLast = X_train[i][:-1]
  X_train1.append(everythingButLast)
X_train1 = np.array(X_train1)
X_test1 = []
for j in range(len(X_test)):
  X_test1.append(X_test[j][:-1])
X_test1 = np.array(X_test1)
svm = SVC()
svm.fit(X_train1, y_train)
svmResults = svm.predict(X_test1)
matrix = confusion_matrix(y_test, svmResults)
accuracies = matrix.diagonal()/matrix.sum(axis = 1)
print("Accuracies per class for our TRADITIONAL subtypes: ", accuracies)
print("Overall accuracy for our TRADITIONAL subtypes: ", accuracy_score(y_test, svmResults))
print("Recall for our TRADITIONAL: ", recall_score(y_test, svmResults, average = 'macro'))
print("Precision for our TRADITIONAL subtypes: ", precision_score(y_test, svmResults, average = 'macro'))
print("Number of points Misclassified per cluster ON AVERAGE: ", mean(findMisclassified(X, X_test1, y, y_test,lstOfIndices)))

# Histogram for class distribution of separated clusters
clusterIter = 0
labelDictionary = {0:"0",1:"1",2:"2",3: "3",4 : "4",5 : "5",6 : "Shirt",7 : "Sneaker",8 : "Bag",9 : "Ankle Boot"}
legend = {0: 'rosybrown', 1:'silver', 2: 'darkred', 3:'coral', 4:'sienna', 5:'peachpuff', 6: 'darkorange', 7:'goldenrod', 8:'darkkhaki', 9:'olive', 10:'darkolivegreen', 11:'lawngreen', 12:'darkseagreen', 13:'aquamarine', 14:'darkslategray', 15:'cyan', 16:'dodgerblue', 17:'cornflowerblue', 18:'indigo', 19:'fuchsia', 20:'palevioletred'}


plt.figure(1)
for cluster in separatedClusters:
  print("this is the class distribution for cluster: {}\n".format(clusterIter))
  print("this is the homogeneity Score for this cluster: {}\n".format(scores[clusterIter]))
  numLabelPerCluster = {}
  for i in cluster: #counting number of points per class in cluster
    label = y[i]
    if label in numLabelPerCluster:
      numLabelPerCluster[label] += 1
    else:
      numLabelPerCluster[label] = 1
  
  classesPerCluster = []
  percentageOfClasses = []
  for key in numLabelPerCluster:
    percentage = (numLabelPerCluster[key]/len(cluster))*100
    print("Number of points of class {} in cluster {}: {} ({})".format(labelDictionary[key],clusterIter,numLabelPerCluster[key],percentage))
    classesPerCluster.append(labelDictionary[key])
    percentageOfClasses.append(percentage)
  plt.subplot()
  plt.xticks()
  plt.bar(classesPerCluster, percentageOfClasses,width=.7, align='center',color = legend[clusterIter])
  plt.title("Cluster {} (Iterative Clustering)".format(clusterIter))
  plt.xlabel("Classes in Cluster {}".format(clusterIter))
  plt.ylabel("Percent of Cluster {}".format(clusterIter)) 

  plt.show()

  clusterIter += 1

#Class Distribution for unclustered data from iterative clustering
numLabelsPerClass = {}
for i in remainingPoints:
  label = y[i]
  if label in numLabelsPerClass:
    numLabelsPerClass[label] += 1
  else:
    numLabelsPerClass[label] = 1

classesInRemaining = []
percentagesPerClassForRemaining = []
for key in numLabelsPerClass:
  percentage = (numLabelsPerClass[key]/len(remainingPoints))*100
  print("Number of points of class {} in remaining: {} ({})".format(labelDictionary[key],numLabelsPerClass[key],percentage))
  classesInRemaining.append(labelDictionary[key])
  percentagesPerClassForRemaining.append(percentage)


plt.bar(classesInRemaining, percentagesPerClassForRemaining,align='center')
plt.title("Unclustered Data Remaining from Iterative Clustering")
plt.xlabel("Classes in Unclustered Data")
plt.ylabel("Percent of Unclustered Data")
plt.xticks(rotation=45, rotation_mode='anchor')